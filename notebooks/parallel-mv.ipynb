{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67be6ddc-7a97-4022-9004-60e526f6ec3e",
   "metadata": {},
   "source": [
    "## MPI parallel matrix-vector product in dolfinx/python\n",
    "\n",
    "In this notebook we implement matrix-vector multiplication with MPI. \n",
    "\n",
    "1. For both matrices and vectors, we construct index maps and use that to distribute values. Basically, we create an IndexMap to describe the distribution scheme we want, and then we use the IndexMap to create objects (such as vectors and matrices) that have this distribution. \n",
    "\n",
    "2. After we define IndexMap and distributed objects we need, we show some basic arithmetic with those objects, such as adding vectors and matrix-vector product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c213f3-9d8d-41c3-9b4d-0623147d241d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee897cea761743d681a17325a3ba10ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import ipyparallel as ipp\n",
    "\n",
    "# create a cluster\n",
    "cluster = ipp.Cluster(engines=\"mpi\", n=3, log_level=logging.WARNING)\n",
    "rc = cluster.start_and_connect_sync(activate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c770245-6d39-449f-8a28-a6e0aabe62a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] I am rank 0 / 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] I am rank 1 / 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] I am rank 2 / 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# Find out rank, size\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "world_rank = comm.Get_rank()\n",
    "world_size = comm.Get_size()\n",
    "print(f\"I am rank {world_rank} / {world_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee17e3-55fd-45ab-a3d0-caa8a0bbd73a",
   "metadata": {},
   "source": [
    "We want to create an overlapping, ie ghosted, index map for our vectors and matrices. (see [dolfinx implementation](https://github.com/FEniCS/dolfinx/blob/160ed13eb476df99944072aec70bd46a6fcb9bcf/cpp/dolfinx/common/IndexMap.cpp))\n",
    "\n",
    "What we need is:\n",
    "- MPI communicator\n",
    "- **source ranks** - ranks that own indices ghosted by the caller\n",
    "- **destination ranks** - ranks that ghost indices owned by the caller\n",
    "- local size of the index map (number of owned entries)\n",
    "- global indices of ghost entries\n",
    "- owner rank of each ghost entry on global communicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e9048e-f418-461b-bdd6-de02e4c97408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_sorted(lst):\n",
    "    return all(a <= b for a, b in zip(lst, lst[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85aad8af-abbb-4152-ac4b-05b99cf6600f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "class IndexMap():\n",
    "    def __init__(self, comm, local_size, ghosts, owners):\n",
    "        # fixme: perhaps hide these and use special methods to access\n",
    "        # assume ghosts and owners are of type list\n",
    "        assert len(ghosts) == len(owners)\n",
    "        \n",
    "        self.comm = comm\n",
    "        self.forward_comm = None\n",
    "        self.reverse_comm = None\n",
    "        self.ghosts = ghosts\n",
    "        self.owners = owners\n",
    "        self.local_size = local_size\n",
    "        \n",
    "        # Get global size\n",
    "        self.global_size = 0\n",
    "        self.compute_size_global()\n",
    "        \n",
    "        # Get global offset (index) using global exclusive reduction\n",
    "        self.local_range = None\n",
    "        self.compute_local_range()\n",
    "        \n",
    "        # Get sources and destinations\n",
    "        self.sources = None\n",
    "        self.destinations = None\n",
    "        self.compute_src_dest()\n",
    "        \n",
    "    def compute_src_dest(self):\n",
    "        # src = sort(owners)\n",
    "        # remove something unique from src\n",
    "        # dest = sort(MPI.compute_graph_edges_nbx(self.comm, src))\n",
    "        \n",
    "        dest_ranks = np.unique(self.owners)\n",
    "        print(f\"{self.comm.rank=}, {dest_ranks=}\")\n",
    "        self.forward_comm = self.comm.Create_dist_graph([self.comm.rank], [len(dest_ranks)], dest_ranks.tolist(), reorder=False)\n",
    "        src, dest, _ = self.forward_comm.Get_dist_neighbors()\n",
    "        \n",
    "        # recv_size = np.zeros(len(source_ranks), dtype=np.int32)\n",
    "        self.reverse_comm = comm.Create_dist_graph_adjacent(dest, src, reorder=False)\n",
    "        src2, dest2, _ = self.reverse_comm.Get_dist_neighbors()\n",
    "        \n",
    "        print(f\"{src=}, {src2=}\")\n",
    "        print(f\"{dest=}, {dest2=}\")\n",
    "        # assume src and dest are lists\n",
    "        # assert is_sorted(src)\n",
    "        # assert is_sorted(dest)\n",
    "        \n",
    "        self.sources = src\n",
    "        self.destinations = dest\n",
    "        \n",
    "    def local_to_global(self, local_ind):\n",
    "        # todo\n",
    "        global_ind = None\n",
    "        return global_ind\n",
    "    \n",
    "    def compute_size_global(self):\n",
    "        # todo: test\n",
    "        self.global_size = self.comm.allreduce([self.local_size], op=MPI.SUM)\n",
    "        \n",
    "    def compute_local_range(self):\n",
    "        # todo: test\n",
    "        # offset = 0\n",
    "        offset = self.comm.exscan([self.local_size], op=MPI.SUM)\n",
    "        if offset is None:\n",
    "            self.local_range = (0, self.local_size)\n",
    "        else: \n",
    "            print(f\"{offset=}\")\n",
    "            self.local_range = (offset[0], offset[0] + self.local_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4921d4ab-63d8-4f08-ac84-965e01d6b70c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] self.comm.rank=0, dest_ranks=array([1], dtype=int32)\n",
       "src=[1, 2], src2=[1]\n",
       "dest=[1], dest2=[1, 2]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] offset=[50, 150]\n",
       "self.comm.rank=2, dest_ranks=array([0], dtype=int32)\n",
       "src=[], src2=[0]\n",
       "dest=[0], dest2=[]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] offset=[50]\n",
       "self.comm.rank=1, dest_ranks=array([0], dtype=int32)\n",
       "src=[0], src2=[0]\n",
       "dest=[0], dest2=[0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import numpy as np\n",
    "if comm.rank == 0:\n",
    "    IM = IndexMap(comm, 50, range(50, 200), list(np.ones(150, dtype=np.int32)))\n",
    "elif comm.rank == 1:\n",
    "    IM = IndexMap(comm, 150, range(50), list(np.zeros(50, dtype=np.int32)))\n",
    "else:\n",
    "    IM = IndexMap(comm, 33, range(5), list(np.zeros(5, dtype=np.int32)))    \n",
    "    #raise RuntimeError(\"Too many processors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e3eb519-8fda-4274-a56d-59fc5658d29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] 0\n",
       "50\n",
       "(0, 50)\n",
       "[1, 2] [1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] 1\n",
       "150\n",
       "(50, 200)\n",
       "[0] [0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 2\n",
       "33\n",
       "(50, 83)\n",
       "[] [0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# Let's check if everything makes sense\n",
    "print(IM.comm.rank)\n",
    "print(IM.local_size)\n",
    "print(IM.local_range)\n",
    "print(IM.sources, IM.destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c31be5-648c-4138-9146-0316bec51132",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we want to use this index map for dolfinx objects (vectors, matrices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c8eeb31-de46-4765-9997-9c27970683fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dolfinx as dfx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bdf99-4154-40f1-b3b0-1377f4c3a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338b31e-da2b-4f1e-ad21-10a9bbd71876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f3fbe-3904-441b-a47a-44cec53ab2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ba9ed8-70d5-4167-820e-e1b90397dd72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop cluster\n",
    "cluster.stop_cluster_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993320dd-deb4-4744-ba46-18f71319209c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
